{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:03:25.203114Z",
     "start_time": "2019-11-15T14:03:24.211373Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Еще раз про деревья решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../imgs/tree_ex.gif' style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формально, дерево решений - это связный ациклический граф. В нем можно выделить 3 типа вершин:\n",
    "1. Корневая вершина (root node) -  откуда все начинается\n",
    "2. Внутренние вершины (intermediate nodes)\n",
    "3. Листья (leafs) - самые глубокие вершины дерева, в которых содержится \"ответ\"\n",
    "\n",
    "Во внутренней или коневой вершине признак проверяется на некий логический критерий, по результатам которого мы движемся все глубже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Меры неопределенности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $p_k$ - это доля класса $C_k$ в узле дерева $S$.\n",
    "\n",
    "1. Missclassification error  \n",
    "$$I(S) = 1 - \\max\\limits_k p_k $$\n",
    "2. Gini index \n",
    "$$I(S) = 1 - \\sum\\limits_k (p_k)^2 = \\sum\\limits_{k'\\neq k} p_{k'} p_k$$\n",
    "3. Entropy \n",
    "$$I(S) = -\\sum\\limits_k p_k \\log(p_k)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прирост информации\n",
    "\n",
    "Выберем признак $A$ и пороговое значение $t$ на нем таким образом, чтобы уменьшить неопределенность:\n",
    "\n",
    "**Насколько уменьшится неопределенность:** <br/>\n",
    "$$ Gain(S, A) = I(S) - \\left(\\frac{|S_L|}{|S|}\\cdot I(S_L) + \\frac{|S_R|}{|S|}\\cdot I(S_R) \\right),$$ где $S_R$ и $S_L$ - это потомки узла $S$ c объектами, удовлетворяющим соответствующим условиям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../imgs/gain_ex.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Критерии останова (регуляризация)\n",
    "\n",
    "* Никогда\n",
    "* Задать порог по мере неопределенности: $I(S) \\leq \\theta$\n",
    "* Задать порог по размеру узла: $|S| \\leq n$\n",
    "* Задать порог на глубину: $Depth(S) = d$\n",
    "* Задать порог на размер потомков: $|S_L| \\leq n_1 \\& |S_R| \\leq n_2$\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия\n",
    "\n",
    "Для задачи регрессии в качестве меры неопределенности могут выступать\n",
    "\n",
    "* Среднее квадратичное отклонение от среднего\n",
    "$$ I(S) = \\frac{1}{|S|}\\sum\\limits_{i \\in S}(y_i - \\bar{y_S})^2 $$\n",
    "* Среднее абсолютное отклонение от среднего\n",
    "$$ I(S) = \\frac{1}{|S|}\\sum\\limits_{i \\in S}|y_i - \\bar{y_S}| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как определяется ответ?\n",
    "\n",
    "* Классификация\n",
    "    * Класс с большинством в листе\n",
    "    * Доли каждого из классов в листе\n",
    "* Регрессия\n",
    "    * Среднее (медиана) целевой переменной в листе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Важность признаков\n",
    "\n",
    "В деревьях решений производится автоматический отбор признаков.\n",
    "\n",
    "Пусть $v(S)$ - это признак, который использовался для ветвления в узле $S$\n",
    "\n",
    "$$ \\text{imp}(A) = \\sum\\limits_{i: v(S_i) = A} \\frac{|S_i|}{|S|} Gain(S_i, A) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с пропусками\n",
    "\n",
    "1. Удалить объекты\\признаки с пропусками\n",
    "2. Пропущенное значение = отдельная категория\n",
    "3. Вычисление impurity без учета пропуска\n",
    "4. Surrogate split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычислительная сложность\n",
    "\n",
    "#### Обучение\n",
    "* Расчет мер неопределенности для $n$ объектов с $d$ признаками на одном уровне:\n",
    "    * $O(dn)$\n",
    "* Надо делать на каждом уровне дерева. Глубина дерева в сбалансированном случае случае - $\\log{(n)}$\n",
    "    * $O(dn \\log{(n)})$\n",
    "    \n",
    "#### Применение\n",
    "* В худшем случае $O(n)$\n",
    "* В сбалансированном случае $O(\\log{(n)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преимущества / Недостатки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Преимущества **\n",
    "* Простота построения\n",
    "* Интерпретируемость (при небольшой глубине)\n",
    "* Требуются минимальная предобработка признаков\n",
    "* Встроенный отбор признаков\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Недостатки **\n",
    "* Границы строяется только параллельно или перпендикулярно осям признаков\n",
    "* При изменении набора данных надо полностью перестраивать и результат может получится совершенно иным\n",
    "* Жадность построения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что нужно дополнительно понимать про деревья\n",
    "* Любое недвоичное дерево решений можно представить двоичным\n",
    "* Любая линейная комбинация деревьев решений есть дерево решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ансамбли"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть нам даны базовы алгоритмы $a_1(x), a_2(x), ..., a_T(x)$, $a_i: \\mathcal{X} \\to \\mathbb{R}$.\n",
    "\n",
    "**Ансамблем** будем называть функцию \n",
    "$$h(x) = C(a_1(x), a_2(x), ..., a_T(x)),$$\n",
    "$C: \\mathbb{R}^T \\to \\mathcal{Y}$ -- решающее правило (метаалгоритм; то как будем комбинировать ответы базовых алгоритмов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Голосование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простое голосование\n",
    "\n",
    "$$h(x) = \\frac{1}{T} \\sum_{i=1}^T a_i(x)$$\n",
    "\n",
    "<img src=\"../imgs/vot1.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Взвешанное голосование\n",
    "\n",
    "$$h(x) = \\frac{1}{T} \\sum_{i=1}^T b_i a_i(x)$$\n",
    "\n",
    "<img src=\"../imgs/vot2.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Смесь экспертов\n",
    "\n",
    "$$h(x) = \\frac{1}{T} \\sum_{i=1}^T b_i(x) a_i(x)$$\n",
    "\n",
    "<img src=\"../imgs/vot3.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending\n",
    "\n",
    "Обучаем базовые алгоритмы на части данных, потом их ответы на другой части данных используем как признаки для метаалгоритма.\n",
    "\n",
    "<img src=\"../imgs/stacking.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какой недостаток у такого подхода?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "Разбиваем обучающую выборку на фолды. Перебираем фолды, обучаем базовые алгоритмы на всех фолдах кроме текущего, на текущем делаем предсказания обученными алгоритмами. Полученные на каждом фолде предсказания используем как признаки для метаалгоритма.\n",
    "\n",
    "Для предсказания на тесте, обучаем алгоритмы на всем обучающем множестве. Получаем метапризнаки на тесте, применив базовые алгоритмы к тестовым данным. Окончательный результат получаем применив метаалгоритм, обученный на обучающем множестве, к тестовым метапризнакам.\n",
    "\n",
    "<img src=\"../imgs/stacking-2b.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А есть ли недостатки у этого подхода?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более подробнее про последние две техники [тут](https://dyakonov.org/2017/03/10/cтекинг-stacking-и-блендинг-blending/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-variance decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем рассматривать задачу регрессии, будем предпологать, что\n",
    "$$y = f(x) + \\epsilon,\\,\\,\\,\\, \\epsilon \\sim \\mathcal{N}(0, \\sigma).$$\n",
    "\n",
    "Будем искать $h(x)$, приближающую исходную зависимость $f(x)$. Хотим оценить матожидание среднеквадратичной ошибки в некоторой точке $x_0$. Для этого будем сэмплировать $y$ из условного распределения $P(y|x=x_0)$.\n",
    "\n",
    "$$err(x_0) = E_{P(y|x=x_0)}[(y - h(x_0))^2]$$\n",
    "\n",
    "По простому это будет обычное MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним определение дисперсии (вариации):\n",
    "$$Var[y] = E[(y - E[y])^2] = E[y^2] - (E[y])^2.$$\n",
    "Отсюда сразу\n",
    "$$E[y^2] = Var[y] + (E[y])^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также логично предположить, что изначальная зависимость $f(x)$ является детерминированной. Из этого следует, что\n",
    "$$E[f(x_0)] = f(x_0).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выражение для ошибки расписывается в\n",
    "$$err(x_0) = E[y^2] + E[h^2(x_0)] - 2E[yh(x_0)].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распишем отдельно члены последней суммы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E[y^2] = Var[y] + (E[y])^2 = E[(y - E[y])^2] + (E[y])^2 = $$\n",
    "$$= E[(\\, f(x_0) + \\epsilon - f(x_0) \\,)^2] + f(x_0)^2 = E[\\epsilon^2] + f(x_0)^2 =$$\n",
    "$$=Var[\\epsilon] + (E[\\epsilon])^2 + f(x_0)^2 = Var[\\epsilon] + f(x_0)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E[h^2(x_0)] = Var[h(x_0)] + (E[h(x_0)])^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E[yh(x_0)] = E[f(x_0)h(x_0) + \\epsilon h(x_0)] = f(x_0)E[h(x_0)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$err(x_0) = Var[\\epsilon] + \\underline{f(x_0)^2} + Var[h(x_0)] + \\underline{(E[h(x_0)])^2} - \\underline{2f(x_0)E[h(x_0)]} = $$\n",
    "$$= Var[\\epsilon] + Var[h(x_0)] + (f(x_0) - E[h(x_0)])^2 = $$\n",
    "$$= (f(x_0) - E[h(x_0)])^2 + E[(h(x_0) - E[h(x_0)])^2] + Var[\\epsilon] =$$\n",
    "$$= \\text{Bias}^2 + \\text{Variance} + \\text{Noise}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td><img src=\"../imgs/bias-var1.png\" style=\"height:400px\"></td>\n",
    "<td><img src=\"../imgs/bias-var2.png\" style=\"height:400px\"></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "Возмем в качестве базовых алгоритмов решающие деревья. Обучим каждое, а ответы их усредним (простое голосование).\n",
    "\n",
    "Что из этого получится?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Небольшой эксперимент\n",
    "* Какая высота у Эйфелевой башни? (без гугла :))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегии\n",
    "\n",
    "* Bagging = Bootstrap aggregation. Обучаем алгоритмы по случайным подвыборкам размера N, полученным с помощью выбора с возвращением.\n",
    "* RSM = Random Subspace Method. Метод случайных подпространств. Обучаем алгоритмы по случайным подмножествам признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом Random Forest = Деревья + Bagging + RSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:03:26.233535Z",
     "start_time": "2019-11-15T14:03:25.205263Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from ipywidgets import interact, IntSlider\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "def rf_demo(n_est=5):\n",
    "    rf = RandomForestClassifier(random_state=123, n_estimators=n_est)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    X, y = make_moons(noise=0.3, random_state=123)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    x_range = np.linspace(X.min(), X.max(), 100)\n",
    "    xx1, xx2 = np.meshgrid(x_range, x_range)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    \n",
    "    for tree in rf.estimators_:\n",
    "        y_hat = tree.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "        y_hat = y_hat.reshape(xx1.shape)\n",
    "\n",
    "        plt.contourf(xx1, xx2, y_hat, alpha=1.0/n_est)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)\n",
    "    \n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    \n",
    "    plt.title('N estimators = %d' % n_est)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T14:03:26.667116Z",
     "start_time": "2019-11-15T14:03:26.236746Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8812816a89ec4d96937430cd413d3303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_est', max=101, min=1, step=5), Output()), _dom_classes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    fig = interact(rf_demo, n_est=IntSlider(min=1, max=101, value=1, step=5))\n",
    "except:\n",
    "    print('Что-то не так. Посмотрите на доску')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-variance для случайного леса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h(x) = \\frac{1}{T}\\sum_{i=1}^T a_i(x),$$\n",
    "$$Var[a_i(x_0)] = \\sigma^2, \\,\\, E[a_i(x_0)] = \\mu.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "$$\\text{Bias}^2 = (E[h(x_0)] - f(x_0))^2 = (E[a(x_0)] - f(x_0))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "$$\\text{Variance} = E[(h(x_0) - E[h(x_0)])^2] =$$\n",
    "$$= \\frac{1}{T^2}\\sum_{i=1}^T \\sum_{j=1}^T E\\left[\\,(a_i(x_0) - E[a_i(x_0)])\\,(a_j(x_0) - E[a_j(x_0)])\\,\\right]=$$\n",
    "$$=\\frac{1}{T^2}\\sum_{i=1}^T\\left(\\sigma^2 + \\sum_{j \\neq i}\\text{cov}(a_i(x_0), a_j(x_0))\\right) = $$\n",
    "$$=\\frac{1}{T^2}\\sum_{i=1}^T \\left(\\sigma^2 + (T-1)\\sigma^2\\rho \\right)=$$\n",
    "$$=\\frac{\\sigma^2}{T} + \\frac{(T-1)\\sigma^2 \\rho}{T} = \\rho \\sigma^2 + \\sigma^2\\frac{1-\\rho}{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайный лес не переобучается с ростом $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем модель взвешанного голосования:\n",
    "$$h(x) = \\frac{1}{T} \\sum_{i=1}^T b_i a_i(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем новый алгоритм к нашей композиции, минимизируя следующую ошибку:\n",
    "$$err(h) = \\sum_{j=1}^N L\\left(y_i, \\sum_{i=1}^{T-1}b_i a_i(x_j) + b\\cdot a(x_j)\\right) \\to \\min_{a,b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ключевая идея в том, чтобы искать очередной $a_i$, который будет аппроксимировать антиградиент ошибки $err(h_{i-1})$ по параметрам $[h_{i-1}(x_1), ..., h_{i-1}(x_N)]$:\n",
    "$$h_{i}(x_j) = h_{i-1}(x_j) - \\eta \\cdot {err}'(h_{i-1}(x_j)).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм обучения GB\n",
    "\n",
    "1. Инициализируем $h_0(x) = \\arg\\min_{\\gamma}\\sum_{j=1}^N L(y_i, \\gamma)$;\n",
    "\n",
    "2. Для все $i$ от 1 до T:\n",
    "    * Для всех $j=1,...,N$ вычисляем\n",
    "    $$g_{i,j} = -\\left[\\frac{\\partial L(y_i, h(x_j))}{\\partial h(x_j)}\\right]_{h=h_{i-1}};$$\n",
    "    * Строим базовую модель $a_i(x)$, беря в качестве таргета $g_{i,j}$:\n",
    "    $$a_i = \\arg\\min_a \\sum_{j=1}^N (g_{i,j} - a(x_j)^2;$$\n",
    "    * Определить вес $b_i$:\n",
    "    $$b_i = \\arg\\min_b \\sum_{j=1}^N L\\left(y_i, h_{i-1}(x_j) + b\\cdot a_i(x_j)\\right);$$\n",
    "    * Обновить модель:\n",
    "    $$h_i(x) = h_{i-1}(x) + b_i \\cdot a_i(x);$$\n",
    "    \n",
    "3. Возращаем $h_T(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Факты о бустинге\n",
    "* В отличии от бэггинга (RF), который уменьшает только дисперсию модели, бустинг уменьшает также и смещение.\n",
    "* Градиентный бустинг может переобучаться с ростом $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/overfit.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Побаловаться: http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регуляризация\n",
    "* Можно добавлять алгоритмы с некоторым дисконтом (аля скорость спуска $\\eta$).\n",
    "* Использовать Bagging.\n",
    "* Что еще можно придумать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сложность\n",
    "* Вычисление градиента O (g (N ))\n",
    "* Построение модели (дерева) O (HND )\n",
    "* Вычисление предсказания O (N )\n",
    "\n",
    "где H - высота дерева.\n",
    "\n",
    "Построение дерева может быть эффективно распараллелено по фичам. Поэтому в целом быстро."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
